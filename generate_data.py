import tensorflow as tf
import numpy as np
from sklearn.utils import shuffle
import glob
from PIL import Image
import os

load_preprocessed = False # use already preprocessed images that were generated by process_and_save function
dataset_dir_path = "dataset"

class FleuretTaskData:

    def __init__(self, batch_size):
        self.dataset_dir_path = dataset_dir_path
        self.batch_size = batch_size

        # load already preprocessed images, each part contains 10000 examples
        if (load_preprocessed):
            class1_a = np.load("processed_dataset/1/part0.npy")
            class1_b = np.load("processed_dataset/1/part1.npy")
            class2_a = np.load("processed_dataset/1/part3.npy")
            class2_b = np.load("processed_dataset/1/part4.npy")
            self.dataset = np.concatenate((class1_a, class1_b, class2_a, class2_b))
        else:
            with tf.variable_scope('root_cnn'):
                self.input_placeholder = tf.placeholder(tf.float32, shape=(self.batch_size, 32, 32))
                self.processed_images = self.get_cnn_feature_vectors(self.input_placeholder)

            whole_dataset = self.load_images(self.dataset_dir_path + "/1")
            self.dataset = np.concatenate((whole_dataset[:20000], whole_dataset[30000:50000]))

        self.labels = np.concatenate(([0] * 20000, [1] * 20000))

    def load_images(self, images_path):
        print("Loading images...")
        image_names = glob.glob(images_path + "/*.png")
        images = np.array([np.array(Image.open(img).convert("L")) for img in image_names])/255.0
        return images

    def get_cnn_feature_vectors(self, batch):
        extracted_features = []
        for i in range(batch.shape[0]):
            # Input Layer
            input_layer = tf.reshape(batch[i], [1, 32, 32, 1])

            # Convolutional Layer #1
            conv1 = tf.layers.conv2d(
                inputs=input_layer,
                filters=16,
                kernel_size=[3, 3],
                padding="same",
                activation=tf.nn.relu)

            # Pooling Layer #1
            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)

            # Convolutional Layer #2 and Pooling Layer #2
            conv2 = tf.layers.conv2d(
                inputs=pool1,
                filters=32,
                kernel_size=[3, 3],
                padding="same",
                activation=tf.nn.relu)
            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)
            feature_vector = tf.reshape(pool2, [64, 32])
            extracted_features.append(feature_vector)
        return tf.stack(extracted_features)

    def create_directory(self, name):
        try:
            os.mkdir(name)
            print("Directory " + name + " created.")
        except FileExistsError:
            print("Directory " + name + " already exists.")

    # Save processed images locally in 10000-chunks to speed up training
    '''def process_and_save_dataset(self, dirName):
        image_directories = os.listdir(self.dataset_dir_path)
        self.create_directory(dirName)

        with tf.Session() as sess:
            for directory in image_directories:
                images = self.load_images(self.dataset_dir_path + "/" + directory)
                for i in range(len(images) // 10000):
                    processed_images = self.get_cnn_feature_vectors(images[i * 10000:(i+1)*10000])

                    print("Initializing TensorFlow variable...")
                    sess.run(tf.global_variables_initializer())

                    self.create_directory(dirName + "/" + directory)
                    save_path =  dirName + "/" + directory + "/part" + str(i)
                    print("Saving processed images representation...")
                    np.save(save_path, processed_images.eval())'''


    def shuffle_dataset(self):
        self.dataset, self.labels = shuffle(self.dataset, self.labels, random_state=0)
        return self.dataset, self.labels

    def get_batch(self, iter_num, batch_size, epoch):
        upper_index = (iter_num + 1) * batch_size
        labels = np.vstack(self.labels[iter_num * batch_size: upper_index])
        if (load_preprocessed):
            return self.dataset[iter_num * batch_size: upper_index], labels
        else:
            if (epoch == 0 and iter_num == 0):
                self.sess = tf.Session()
                self.sess.run(tf.global_variables_initializer())
            images = self.sess.run([self.processed_images], feed_dict= {self.input_placeholder: self.dataset[iter_num * batch_size: upper_index]})[0]
            return images, labels